{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e2d74baa-ea84-4d16-8013-5b77ecc3316f",
   "metadata": {},
   "source": [
    "# GSC Query Analysis - August 2024 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bbd42f6-8dde-491f-ab1f-2b94eb917198",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud\n",
    "from datetime import datetime\n",
    "from fpdf import FPDF\n",
    "from PyPDF2 import PdfFileMerger\n",
    "from collections import defaultdict, Counter\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff2714b9-6a0b-477d-829f-a92433d28193",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Get branded terms and extract grams, bigrams, and trigrams\n",
    "brand_name = input(\"What is your full brand name? \")\n",
    "brand_name_lower = brand_name.lower()\n",
    "brand_words = brand_name_lower.split()\n",
    "brand_bigrams = [f\"{brand_words[i]} {brand_words[i+1]}\" for i in range(len(brand_words)-1)]\n",
    "brand_trigrams = [f\"{brand_words[i]} {brand_words[i+1]} {brand_words[i+2]}\" for i in range(len(brand_words)-2)]\n",
    "\n",
    "# Combine all branded terms into a single set\n",
    "excluded_terms = set(brand_words + brand_bigrams + brand_trigrams)\n",
    "\n",
    "print(f\"Excluded terms list created: {excluded_terms}\")\n",
    "\n",
    "\n",
    "# Step 1: Get brand name and generate grams\n",
    "#brand_name = input(\"What is your full brand name? \")\n",
    "#brand_name_lower = brand_name.lower()\n",
    "#brand_words = brand_name_lower.split()\n",
    "#brand_bigrams = [f\"{brand_words[i]} {brand_words[i+1]}\" for i in range(len(brand_words)-1)]\n",
    "#brand_trigrams = [f\"{brand_words[i]} {brand_words[i+1]} {brand_words[i+2]}\" for i in range(len(brand_words)-2)]\n",
    "\n",
    "#print(f\"Grams identified: {', '.join(brand_words)}\")\n",
    "#print(f\"Bigrams identified: {', '.join(brand_bigrams)}\")\n",
    "#print(f\"Trigrams identified: {', '.join(brand_trigrams)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a73203a0-c5c6-4c9b-9378-686ae29b2d4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Accept non-branded terms\n",
    "#non_branded_terms = set()\n",
    "#print(\"Are any of these terms non-branded? Enter STOP to stop.\")\n",
    "#while True:\n",
    "    #term = input(\"Enter a term: \").strip().lower()\n",
    "    #if term == 'stop':\n",
    "        #break\n",
    "    #if term in brand_words or any(term in bigram for bigram in brand_bigrams) or any(term in trigram for trigram in brand_trigrams):\n",
    "        #print(\"This term is not one of the branded terms you gave before - try again or STOP!\")\n",
    "    #else:\n",
    "        #non_branded_terms.add(term)\n",
    "\n",
    "\n",
    "# Step 2: Ask for exceptions to the excluded terms list\n",
    "exceptions = set()\n",
    "print(\"Enter exceptions to the excluded terms list. Enter STOP to stop.\")\n",
    "while True:\n",
    "    term = input(\"Enter a term: \").strip().lower()\n",
    "    if term == 'stop':\n",
    "        break\n",
    "    \n",
    "    # Check if the term is an exact match with any of the terms in the excluded list\n",
    "    if term in excluded_terms:\n",
    "        exceptions.add(term)\n",
    "        print(f\"Term '{term}' added as an exception for analysis.\")\n",
    "    else:\n",
    "        print(\"This term is not in the excluded terms list. Try again.\")\n",
    "\n",
    "print(\"Exceptions identified:\", exceptions)\n",
    "\n",
    "# You can now use 'excluded_terms' to filter queries and 'exceptions' to include in the analysis\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc3b6177-8462-4f53-b257-24bed38b3163",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Load the Excel file\n",
    "# make sure you have the queries in column A of the \"query\" tab i.e. standard GSC export\n",
    "file_path = 'path-to-your-excel-download-of-gsc-data-here.xlsx'\n",
    "df = pd.read_excel(file_path, sheet_name='Queries')\n",
    "\n",
    "# Clean up queries\n",
    "def clean_query(query):\n",
    "    query = query.lower()\n",
    "    for term in brand_words + brand_bigrams + brand_trigrams:\n",
    "        query = re.sub(r'\\b' + re.escape(term) + r'\\b', '', query)\n",
    "    query = re.sub(r'\\s+', ' ', query).strip()\n",
    "    return query\n",
    "\n",
    "df['Cleaned Queries'] = df['Top queries'].apply(clean_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a588eb73-cfff-4183-b4b8-207d7272a365",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Co-occurrence Matrix\n",
    "vectorizer = CountVectorizer(stop_words='english')\n",
    "X = vectorizer.fit_transform(df['Cleaned Queries'])\n",
    "X_array = X.toarray()\n",
    "co_occurrence_matrix = np.dot(X_array.T, X_array)\n",
    "co_occurrence_df = pd.DataFrame(co_occurrence_matrix, index=vectorizer.get_feature_names_out(), columns=vectorizer.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d73ec579-2f77-4c4b-8531-7ddfe6abeb90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Co-occurrence Matrix\n",
    "plt.figure(figsize=(12, 10))\n",
    "sns.heatmap(co_occurrence_df, cmap='viridis', annot=False, fmt='d')\n",
    "plt.title('Co-occurrence Matrix')\n",
    "heatmap_path = f\"{brand_name} GSC research {datetime.now().strftime('%Y-%m-%d')}.pdf\"\n",
    "plt.savefig(heatmap_path, format='pdf')\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81979c1e-b113-4102-8728-3df66920653d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word Cloud\n",
    "text = ' '.join(df['Cleaned Queries'])\n",
    "wordcloud = WordCloud(width=800, height=400, background_color='white', stopwords=set(vectorizer.get_stop_words())).generate(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7272b30-c5c3-4079-9b88-101cd3daa485",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Word Cloud\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis('off')\n",
    "wordcloud_path = f\"{brand_name} GSC research {datetime.now().strftime('%Y-%m-%d')}.pdf\"\n",
    "plt.savefig(wordcloud_path, format='pdf')\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "295ada87-818b-4495-9781-792dab047d94",
   "metadata": {},
   "outputs": [],
   "source": [
    "#define paths for output PNG files\n",
    "heatmap_path = f\"{brand_name} GSC research {datetime.now().strftime('%Y-%m-%d')}_heatmap.png\"\n",
    "wordcloud_path = f\"{brand_name} GSC research {datetime.now().strftime('%Y-%m-%d')}_wordcloud.png\"\n",
    "\n",
    "# Ensure necessary downloads\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Step 1: Read the Excel file\n",
    "file_path = 'heroeslawncare.com-queries-meeting-followup-aug-5-2024.xlsx'\n",
    "df = pd.read_excel(file_path, sheet_name='Queries')\n",
    "\n",
    "# Step 2: Combine the relevant text columns (assuming 'Top queries' contains the text data)\n",
    "combined_text = ' '.join(df['Top queries'].astype(str))\n",
    "\n",
    "# Step 3: Define additional stopwords for this analysis\n",
    "brand_name = \"Heroes Lawn Care Lawncare\"  # Replace with your actual brand name\n",
    "brand_name_lower = brand_name.lower()\n",
    "brand_words = brand_name_lower.split()\n",
    "additional_excluded_keywords = brand_words + [\n",
    "    'one', 'would', 'like', 'is', 'i', 'hello', \n",
    "    'heroes', 'lawn', 'care', 'lawncare', 'heroeslawncare.com'\n",
    "]\n",
    "\n",
    "# Step 4: Tokenize and filter the text data\n",
    "stop_words = set(stopwords.words('english'))\n",
    "tokens = word_tokenize(combined_text)\n",
    "filtered_tokens = [word.lower() for word in tokens if word.isalnum() and word.lower() not in stop_words and word.lower() not in additional_excluded_keywords]\n",
    "\n",
    "# Step 5: Get the top 20 keywords\n",
    "top_20_keywords = [item[0] for item in Counter(filtered_tokens).most_common(20)]\n",
    "\n",
    "# Step 6: Calculate the co-occurrence matrix for the top 20 keywords\n",
    "co_occurrence_matrix = defaultdict(lambda: defaultdict(int))\n",
    "window_size = 5\n",
    "\n",
    "for i in range(len(filtered_tokens) - window_size + 1):\n",
    "    window = filtered_tokens[i:i+window_size]\n",
    "    for j, word1 in enumerate(window):\n",
    "        if word1 in top_20_keywords:\n",
    "            for k in range(j+1, len(window)):\n",
    "                word2 = window[k]\n",
    "                if word2 in top_20_keywords:\n",
    "                    co_occurrence_matrix[word1][word2] += 1\n",
    "                    co_occurrence_matrix[word2][word1] += 1\n",
    "\n",
    "# Step 7: Convert the co-occurrence matrix to a pandas DataFrame\n",
    "co_occurrence_df = pd.DataFrame(co_occurrence_matrix).fillna(0)\n",
    "\n",
    "# Step 8: Visualize the co-occurrence matrix using a heatmap with annotations\n",
    "heatmap_path = f\"{brand_name} GSC research {datetime.now().strftime('%Y-%m-%d')}_heatmap.png\"\n",
    "plt.figure(figsize=(12, 10))\n",
    "sns.heatmap(co_occurrence_df, annot=True, cmap='YlGnBu', fmt='d', annot_kws={\"size\": 8}, cbar_kws={'label': 'Frequency'})\n",
    "plt.title('Keyword Co-occurrence Matrix')\n",
    "plt.xticks(rotation=90)\n",
    "plt.yticks(rotation=0)\n",
    "plt.tight_layout()  # Ensure everything fits without overlap\n",
    "plt.savefig(heatmap_path, format='png')\n",
    "plt.close()\n",
    "\n",
    "# Save word cloud as PNG\n",
    "wordcloud_path = f\"{brand_name} GSC research {datetime.now().strftime('%Y-%m-%d')}_wordcloud.png\"\n",
    "wordcloud_text = ' '.join(filtered_tokens)\n",
    "wordcloud = WordCloud(width=800, height=400, background_color='white').generate(wordcloud_text)\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis('off')\n",
    "plt.savefig(wordcloud_path, format='png')\n",
    "plt.close()\n",
    "\n",
    "# Create PDF with FPDF\n",
    "pdf = FPDF()\n",
    "\n",
    "# Add Co-occurrence Matrix page\n",
    "pdf.add_page()\n",
    "pdf.set_font(\"Arial\", size = 12)\n",
    "pdf.cell(200, 10, txt = \"Co-occurrence Matrix\", ln = True, align = 'C')\n",
    "\n",
    "# Add refined description for Co-occurrence Matrix\n",
    "pdf.set_font(\"Arial\", size = 10)\n",
    "pdf.ln(10)  # Add a line break for spacing\n",
    "pdf.multi_cell(0, 10, txt=\"A co-occurrence matrix displays the frequency with which pairs of top keywords appear together within a specified window of text. Each cell in the matrix represents the number of times a specific pair of keywords co-occurs in the data, helping to identify common word associations.\")\n",
    "\n",
    "# Add the co-occurrence matrix image\n",
    "pdf.image(heatmap_path, x = 10, y = 60, w = 180)  # Adjust y and w as needed\n",
    "\n",
    "# Add Word Cloud page\n",
    "pdf.add_page()\n",
    "pdf.set_font(\"Arial\", size = 12)\n",
    "pdf.cell(200, 10, txt = \"Word Cloud\", ln = True, align = 'C')\n",
    "\n",
    "# Add description for Word Cloud\n",
    "pdf.set_font(\"Arial\", size = 10)\n",
    "pdf.ln(10)  # Add a line break for spacing\n",
    "pdf.multi_cell(0, 10, txt=\"A word cloud is a visual representation of words where the size of each word indicates its frequency or importance. In this word cloud, larger words appear more frequently in the data, highlighting the most common terms.\")\n",
    "\n",
    "# Add the word cloud image\n",
    "pdf.image(wordcloud_path, x = 10, y = 60, w = 180)  # Adjust y and w as needed\n",
    "\n",
    "# Save the combined PDF\n",
    "combined_pdf_path = f\"{brand_name} GSC research {datetime.now().strftime('%Y-%m-%d')}.pdf\"\n",
    "pdf.output(combined_pdf_path)\n",
    "\n",
    "print(f\"PDF saved as {combined_pdf_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24e1f062-23aa-48f4-b4a2-7137ef617492",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up intermediate files\n",
    "import os\n",
    "os.remove(heatmap_path)\n",
    "os.remove(wordcloud_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
